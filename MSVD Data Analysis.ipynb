{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc4f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import random\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN , Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0a7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n",
      "1702\n",
      "1200\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1970"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 02. Train & Test Split Data\n",
    "\n",
    "with open('./training_data/AllVideoDescriptions.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "all_videos = list(set([l.split(' ')[0] for l in lines]))\n",
    "\n",
    "with open('./train_split.txt','r') as f:\n",
    "    train_l = f.readlines()\n",
    "train_l = [l.replace('\\n','') for l in train_l]\n",
    "\n",
    "with open('./test_split.txt','r') as f:\n",
    "    test_l = f.readlines()\n",
    "test_l = [l.replace('\\n','') for l in test_l]\n",
    "\n",
    "with open('./val_split.txt','r') as f:\n",
    "    val_l = f.readlines()\n",
    "val_l = [l.replace('\\n','') for l in val_l]\n",
    "\n",
    "len(train_l), len(test_l) , len(val_l)\n",
    "\n",
    "scripts = {}\n",
    "for l in lines:\n",
    "    id = l.split(' ')[0]\n",
    "    script = ' '.join(l.split(' ')[1:])\n",
    "    if id in scripts:\n",
    "      scripts[id].append(script)\n",
    "    else:\n",
    "      scripts[id] = [script]\n",
    "\n",
    "training_list = []\n",
    "validation_list = []\n",
    "test_list = [] \n",
    "vocab_list = []\n",
    "\n",
    "for y in train_l:\n",
    "  for caption in scripts[y][0:1]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    training_list.append([caption, y])\n",
    "\n",
    "for y in val_l:\n",
    "  for caption in scripts[y][0:1]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    validation_list.append([caption, y])\n",
    "    \n",
    "for y in test_l:\n",
    "  for caption in scripts[y][0:1]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    test_list.append([caption, y])\n",
    "\n",
    "train_list = training_list + validation_list + test_list\n",
    "\n",
    "print(len(train_list))\n",
    "random.shuffle(training_list)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(test_list)\n",
    "\n",
    "for train in training_list:\n",
    "    vocab_list.append(train[0])\n",
    "# Tokenizing the words\n",
    "tokenizer = Tokenizer(num_words=1500)\n",
    "tokenizer.fit_on_texts(vocab_list)\n",
    "print(len(tokenizer.word_index))\n",
    "x_data = {}\n",
    "TRAIN_FEATURE_DIR = os.path.join('training_data', 'features_dir')\n",
    "# Loading all the numpy arrays at once and saving them in a dictionary\n",
    "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
    "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
    "    x_data[filename[:-4]] = f\n",
    "print(len(training_list))\n",
    "print(len(validation_list))\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7533bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of video:1970\n",
      "Average description # per video:41.028934010152284\n",
      "All description # per video:80827\n",
      "Number of Token: 1702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of video:{len(all_videos)}\")\n",
    "\n",
    "\n",
    "print(f\"Average description # per video:{np.mean([len(scripts[k]) for k in scripts])}\")\n",
    "print(f\"All description # per video:{sum([len(scripts[k]) for k in scripts])}\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(vocab_list)\n",
    "print(f\"Number of Token: {len(tokenizer.index_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d301f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

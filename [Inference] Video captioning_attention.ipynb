{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc4f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import shutil\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import random\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN , Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9d15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    train_path = \"./training_data\"\n",
    "    test_path = \"./testing_data\"\n",
    "    batch_size = 160\n",
    "    learning_rate = 0.0007\n",
    "    epochs = 30\n",
    "    latent_dim = 512\n",
    "    num_encoder_tokens = 4096\n",
    "    num_decoder_tokens = 1500\n",
    "    time_steps_encoder = 80\n",
    "    max_probability = -1\n",
    "    validation_split = 0.15\n",
    "    max_length = 30\n",
    "    search_type = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1852919",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a250aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import shutil\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import random\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN , Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ae8ca",
   "metadata": {},
   "source": [
    "## 02. Train & Test Dat Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da64663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./training_data/AllVideoDescriptions.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24945f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_split.txt','r') as f:\n",
    "    train_l = f.readlines()\n",
    "train_l = [l.replace('\\n','') for l in train_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78d99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_split.txt','r') as f:\n",
    "    test_l = f.readlines()\n",
    "test_l = [l.replace('\\n','') for l in test_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85852b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./val_split.txt','r') as f:\n",
    "    val_l = f.readlines()\n",
    "val_l = [l.replace('\\n','') for l in val_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512ca452",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = {}\n",
    "for l in lines:\n",
    "    id = l.split(' ')[0]\n",
    "    script = ' '.join(l.split(' ')[1:])\n",
    "    if id in scripts:\n",
    "      scripts[id].append(script)\n",
    "    else:\n",
    "      scripts[id] = [script]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66333896",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = []\n",
    "validation_list = []\n",
    "test_list = [] \n",
    "vocab_list = []\n",
    "\n",
    "for y in train_l:\n",
    "  for caption in scripts[y]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    training_list.append([caption, y])\n",
    "\n",
    "for y in val_l:\n",
    "  for caption in scripts[y]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    validation_list.append([caption, y])\n",
    "    \n",
    "for y in test_l:\n",
    "  for caption in scripts[y]:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "\n",
    "    test_list.append([caption, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16814fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = training_list + validation_list + test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a11f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80827\n",
      "9463\n",
      "48774\n",
      "4290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1970"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_list))\n",
    "random.shuffle(training_list)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(test_list)\n",
    "\n",
    "for train in training_list:\n",
    "    vocab_list.append(train[0])\n",
    "# Tokenizing the words\n",
    "tokenizer = Tokenizer(num_words=1500)\n",
    "tokenizer.fit_on_texts(vocab_list)\n",
    "print(len(tokenizer.word_index))\n",
    "x_data = {}\n",
    "TRAIN_FEATURE_DIR = os.path.join('training_data', 'features_dir')\n",
    "# Loading all the numpy arrays at once and saving them in a dictionary\n",
    "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
    "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
    "    x_data[filename[:-4]] = f\n",
    "print(len(training_list))\n",
    "print(len(validation_list))\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9483e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom data generator because we cannot load so many files at once\n",
    "def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=1500,training_list=training_list, batch_size=32, maxlen=30):\n",
    "    encoder_input_data = []\n",
    "    decoder_input_data = []\n",
    "    decoder_target_data = []\n",
    "    videoId = []\n",
    "    videoSeq = []\n",
    "    # separating the videoId and the video captions\n",
    "    for idx, cap in enumerate(training_list):\n",
    "        caption = cap[0]\n",
    "        videoId.append(cap[1])\n",
    "        videoSeq.append(caption)\n",
    "    # converting the captions to tokens and padding them to equal sizes\n",
    "    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
    "    train_sequences = np.array(train_sequences)\n",
    "    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
    "    max_seq_length = train_sequences.shape[1]\n",
    "    filesize = len(train_sequences)\n",
    "    vCount = 0\n",
    "    n = 0\n",
    "    for i in range(epochs):\n",
    "      for idx in  range(0,filesize):\n",
    "        n += 1\n",
    "        try:\n",
    "          encoder_input_data.append(x_data[videoId[idx]])\n",
    "        except:\n",
    "          continue\n",
    "        y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
    "        decoder_input_data.append(y[:-1])\n",
    "        decoder_target_data.append(y[1:])\n",
    "        if n == batch_size:\n",
    "          encoder_input = np.array(encoder_input_data)\n",
    "          decoder_input = np.array(decoder_input_data)\n",
    "          decoder_target = np.array(decoder_target_data)\n",
    "          encoder_input_data = []\n",
    "          decoder_input_data = []\n",
    "          decoder_target_data = []\n",
    "          n = 0\n",
    "          yield ([encoder_input, decoder_input], decoder_target)\n",
    "            \n",
    "# writing the train and validation generator\n",
    "train = load_datatest(train_path='training_data',batch_size=320, training_list=training_list, x_data=x_data, epochs=150)\n",
    "valid = load_datatest(train_path='training_data',batch_size=320, training_list=validation_list, x_data=x_data, epochs=150)\n",
    "test = load_datatest(train_path='test_data',batch_size=320, training_list=test_list, x_data=x_data, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a6d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ff831",
   "metadata": {},
   "source": [
    "# 03. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39da7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim,time_steps_encoder,num_encoder_tokens):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = tf.keras.layers.LSTM( \n",
    "            hidden_dim, return_sequences=True, return_state=True, input_shape=(time_steps_encoder,num_encoder_tokens))\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "        # Call the LSTM unit\n",
    "        output, state_h, state_c = self.lstm(input_sequence, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        # Return a all 0s initial states\n",
    "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
    "                tf.zeros([batch_size, self.hidden_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b183df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, rnn_size, attention_func):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_func = attention_func\n",
    "\n",
    "        if attention_func not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(\n",
    "                'Attention score must be either dot, general or concat.')\n",
    "\n",
    "        if attention_func == 'general':\n",
    "            # General score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
    "        elif attention_func == 'concat':\n",
    "            # Concat score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "            self.va = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        if self.attention_func == 'dot':\n",
    "            # Dot score function: decoder_output (dot) encoder_output\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n",
    "        elif self.attention_func == 'general':\n",
    "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, self.wa(\n",
    "                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n",
    "        elif self.attention_func == 'concat':\n",
    "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
    "            # Decoder output must be broadcasted to encoder output's shape first\n",
    "            decoder_output = tf.tile(\n",
    "                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n",
    "\n",
    "            # Concat => Wa => va\n",
    "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
    "            score = self.va(\n",
    "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n",
    "\n",
    "            # Transpose score vector to have the same shape as other two above\n",
    "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
    "            score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n",
    "\n",
    "        # alignment a_t = softmax(score)\n",
    "        alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n",
    "        \n",
    "        # context vector c_t is the weighted average sum of encoder output\n",
    "        context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516c97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size,hidden_dim,time_steps_encoder,num_encoder_tokens,attention_func):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = LuongAttention(hidden_dim, attention_func)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True,input_shape=(None,num_encoder_tokens))\n",
    "        self.wc = tf.keras.layers.Dense(hidden_dim, activation='tanh')\n",
    "        self.ws = tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
    "        \n",
    "    def call(self, input_sequence, state,encoder_output):\n",
    "        # Call the LSTM unit\n",
    "        lstm_out, state_h, state_c = self.lstm(input_sequence, state)\n",
    "        context , alignment = self.attention(lstm_out,encoder_output)\n",
    "        lstm_out = tf.concat(\n",
    "        [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
    "        lstm_out = self.wc(lstm_out)\n",
    "        logits = self.ws(lstm_out)\n",
    "\n",
    "\n",
    "        return logits, state_h, state_c, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fb8fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2445259/2347660553.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_sequences = np.array(train_sequences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.28630525 0.         0.         ... 0.         0.         5.2103844 ]\n",
      "  [0.21659842 0.         0.         ... 0.         0.03303999 4.683123  ]\n",
      "  [1.072902   0.         0.         ... 0.         0.         4.327787  ]\n",
      "  ...\n",
      "  [0.6158212  0.         0.         ... 0.         0.         6.1895328 ]\n",
      "  [0.6505584  0.         0.         ... 0.         0.         6.0730615 ]\n",
      "  [0.         0.         0.         ... 0.         0.         6.9953475 ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         2.0637891  1.4911273 ]\n",
      "  [0.         0.         0.         ... 0.         1.2037091  1.4529827 ]\n",
      "  [0.         0.         0.         ... 0.         0.         1.7469425 ]\n",
      "  ...\n",
      "  [0.         0.         1.0136279  ... 0.         0.2805397  0.        ]\n",
      "  [0.         0.         0.9953279  ... 0.0639399  0.2224734  0.        ]\n",
      "  [0.         0.         0.77224773 ... 0.25638026 0.13217995 0.        ]]\n",
      "\n",
      " [[0.         0.         0.34780604 ... 0.         2.3444505  0.69620126]\n",
      "  [0.         0.         0.         ... 0.         3.9255927  0.39716113]\n",
      "  [0.         0.         0.13605541 ... 0.         3.771259   0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         3.1397493  1.6904619 ]\n",
      "  [0.         0.         0.         ... 0.         3.3318932  1.4402584 ]\n",
      "  [0.         0.         0.         ... 0.         1.4236815  0.6659067 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         4.3889585 ]\n",
      "  [0.         0.         0.         ... 0.         0.         5.3501844 ]\n",
      "  [0.         0.10482278 0.         ... 0.         0.         2.990236  ]\n",
      "  ...\n",
      "  [0.         0.         0.32756764 ... 0.         0.         3.4544296 ]\n",
      "  [0.         1.6033926  0.         ... 0.         0.         0.6479458 ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.7022796 ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         1.3457313 ]\n",
      "  [0.         0.         0.         ... 0.         0.         1.2841799 ]\n",
      "  [0.         0.24372557 0.         ... 0.         0.8822562  1.3119881 ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.0949989 ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.73003566]\n",
      "  [0.2339096  0.         0.         ... 0.         0.         1.6486452 ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.14644653]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.2646463 ]\n",
      "  ...\n",
      "  [0.         1.9264901  1.1408454  ... 0.         0.         0.        ]\n",
      "  [0.         1.5420249  0.9805815  ... 0.         0.         0.        ]\n",
      "  [0.         1.346105   0.98812425 ... 0.         0.         0.        ]]] [[[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]] [[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 12:49:01.700186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.701070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.707002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.707576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.708122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.708618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:01.709656: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 12:49:02.003426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.004070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.004720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.005299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.005934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.006616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.703536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.704084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.704593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.705052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.705548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.706015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7409 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0d:00.0, compute capability: 7.5\n",
      "2022-06-09 12:49:02.706325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 12:49:02.706798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9645 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:0e:00.0, compute capability: 7.5\n",
      "2022-06-09 12:49:03.521222: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './model_att_general_2022-06-02 '\n",
    "#Set the length of the input and output vocabulary\n",
    "time_steps_encoder=80\n",
    "num_encoder_tokens=4096\n",
    "latent_dim=512\n",
    "time_steps_decoder=10\n",
    "num_decoder_tokens=1500\n",
    "batch_size=320\n",
    "HIDDEN_DIM =  latent_dim\n",
    "\n",
    "for ei,di in train:\n",
    "    print(ei[0],ei[1],di)\n",
    "    break\n",
    "\n",
    "encoder = Encoder(HIDDEN_DIM,time_steps_encoder,num_encoder_tokens)\n",
    "output =encoder(ei[0],encoder.init_states(batch_size))\n",
    "encoder.load_weights(os.path.join(save_model_path, 'encoder.h5'))\n",
    "\n",
    "decoder = Decoder(num_decoder_tokens,HIDDEN_DIM,time_steps_decoder,num_encoder_tokens,'general')\n",
    "de_input = np.zeros((320, 1, 1500))\n",
    "de_idx = tokenizer.word_index['bos']\n",
    "de_state_h, de_state_c = output[1:]\n",
    "decoder(de_input, (de_state_h, de_state_c), output[0])\n",
    "decoder.load_weights(os.path.join(save_model_path, 'decoder.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96320",
   "metadata": {},
   "source": [
    "# 04. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to perform inference on all test files and save as test_output.txt\n",
    "class Video2Text(object):\n",
    "    ''' Initialize the parameters for the model '''\n",
    "    def __init__(self,encoder,decoder,tokenizer,save_model_path):\n",
    "        self.latent_dim = 512\n",
    "        self.num_encoder_tokens = 4096\n",
    "        self.num_decoder_tokens = 1500\n",
    "        self.time_steps_encoder = 80\n",
    "        self.time_steps_decoder = None\n",
    "        self.preload = True\n",
    "        self.preload_data_path = 'preload_data'\n",
    "        self.max_probability = -1\n",
    "\n",
    "        # processed data\n",
    "        self.encoder_input_data = []\n",
    "        self.decoder_input_data = []\n",
    "        self.decoder_target_data = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # models\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self.inf_encoder_model = None\n",
    "        self.inf_decoder_model = None\n",
    "        self.save_model_path = save_model_path\n",
    "        self.test_path = 'testing_data'\n",
    "        self.inf_encoder_model = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    \n",
    "    def decode_sequence2bs(self, input_seq):\n",
    "        en_initial_states = self.inf_encoder_model.init_states(1)\n",
    "        output, state_h,state_c = self.inf_encoder_model(input_seq,en_initial_states)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        self.beam_search(target_seq, [state_h,state_c],[],[],[],0,output)\n",
    "        return decode_seq, decode_att\n",
    "\n",
    "    def beam_search(self, target_seq, states_value, prob,  path, att, lens,en_output):\n",
    "        global decode_seq\n",
    "        global decode_att\n",
    "        node = 2\n",
    "        \n",
    "        output_tokens, de_state_h, de_state_c, alignment = self.decoder(\n",
    "                    target_seq, states_value, en_output)\n",
    "        output_tokens = tf.reshape(output_tokens,self.num_decoder_tokens)#.reshape((self.num_decoder_tokens))\n",
    "\n",
    "        sampled_token_index = tf.argsort(output_tokens,-1).numpy()[-50:][::-1]       \n",
    "        states_value = [de_state_h, de_state_c]\n",
    "        for i in range(node):\n",
    "            if sampled_token_index[i] == 0:\n",
    "                sampled_char = ''\n",
    "            else:\n",
    "                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
    "            MAX_LEN = 15\n",
    "            if(sampled_char != 'eos' and lens <= MAX_LEN):\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                if(sampled_char == ''):\n",
    "                    p = 1\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                path_new = list(path)\n",
    "                path_new.append(sampled_char)\n",
    "                att_new = list(att)\n",
    "                att_new.append(alignment)\n",
    "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
    "                self.beam_search(target_seq, states_value, prob_new, path_new, att_new, lens+1,en_output)\n",
    "            else:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                p = functools.reduce(operator.mul, prob_new, 1)\n",
    "                if(p > self.max_probability):\n",
    "                    decode_seq = path\n",
    "                    decode_att = att\n",
    "                    self.max_probability = p\n",
    "\n",
    "    def decoded_sentence_tuning(self, decoded_sentence):\n",
    "        decode_str = []\n",
    "        filter_string = ['bos', 'eos']\n",
    "        unigram = {}\n",
    "        last_string = \"\"\n",
    "        for idx2, c in enumerate(decoded_sentence):\n",
    "            if c in unigram:\n",
    "                unigram[c] += 1\n",
    "            else:\n",
    "                unigram[c] = 1\n",
    "            if(last_string == c and idx2 > 0):\n",
    "                continue\n",
    "            if c in filter_string:\n",
    "                continue\n",
    "            if len(c) > 0:\n",
    "                decode_str.append(c)\n",
    "            if idx2 > 0:\n",
    "                last_string = c\n",
    "        return decode_str\n",
    "\n",
    "    def get_test_data(self, path):\n",
    "        X_test = []\n",
    "        X_test_filename = []\n",
    "#         with open (os.path.join(path, 'testing_id.txt')) as testing_file:\n",
    "#             lines = testing_file.readlines()\n",
    "#             for filename in lines:\n",
    "#                 filename = filename.strip()\n",
    "#                 f = np.load(os.path.join(path , 'feat', filename + '.npy'))\n",
    "#                 X_test.append(f)\n",
    "#                 X_test_filename.append(filename[:-4])\n",
    "#             X_test = np.array(X_test)\n",
    "        for filename in test_l:\n",
    "            try:\n",
    "                f = np.load(os.path.join(path , 'features_dir', filename + '.npy'))\n",
    "            except: \n",
    "                print('not exsits')\n",
    "                continue\n",
    "            X_test.append(f)\n",
    "            X_test_filename.append(filename)\n",
    "        return X_test, X_test_filename\n",
    "\n",
    "    def test(self,method='beam'):\n",
    "        X_test, X_test_filename = self.get_test_data(os.path.join(config.train_path)) \n",
    "        self.X_test , self.X_test_filename = X_test, X_test_filename\n",
    "        # generate inference test outputs\n",
    "        with open(os.path.join(self.save_model_path, f'test_output_{method}.txt'), 'w') as file:\n",
    "            for idx, x in enumerate(X_test): \n",
    "                if method =='beam':\n",
    "                    file.write(X_test_filename[idx]+',')\n",
    "                    decoded_sentence,_ = self.decode_sequence2bs(x.reshape(-1, 80, 4096))\n",
    "                    decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
    "                    for d in decode_str:\n",
    "                        file.write(d + ' ')\n",
    "                elif method =='topk':\n",
    "                    file.write(X_test_filename[idx]+',')\n",
    "                    decoded_sentence = self.top_k_sampling(x.reshape(-1, 80, 4096))\n",
    "                    file.write(decoded_sentence)\n",
    "                file.write('\\n')\n",
    "                # re-init max prob\n",
    "                self.max_probability = -1\n",
    "                \n",
    "    def index_to_word(self):\n",
    "        # inverts word tokenizer\n",
    "        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n",
    "        return index_to_word\n",
    "                \n",
    "    def greedy_search(self, f):\n",
    "        \"\"\"\n",
    "                :param f: the loaded numpy array after creating videos to frames and extracting features\n",
    "                :return: the final sentence which has been predicted greedily\n",
    "                \"\"\"\n",
    "        inv_map = self.index_to_word()\n",
    "        en_initial_states = self.inf_encoder_model.init_states(1)\n",
    "        output, state_h,state_c = self.inf_encoder_model(f.reshape(-1, 80, 4096),en_initial_states)\n",
    "        target_seq = np.zeros((1, 1, 1500))\n",
    "        sentence = ''\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        for i in range(15):\n",
    "            output_tokens, state_h, state_c, alignment = self.decoder(target_seq, [state_h,state_c],output)\n",
    "            output_tokens = tf.reshape(output_tokens,self.num_decoder_tokens)#.reshape((self.num_decoder_tokens))\n",
    "            y_hat = np.argmax(output_tokens)\n",
    "            if y_hat == 0:\n",
    "                continue\n",
    "            if inv_map[y_hat] is None:\n",
    "                break\n",
    "            else:\n",
    "                sentence = sentence + inv_map[y_hat] + ' '\n",
    "                target_seq = np.zeros((1, 1, 1500))\n",
    "                target_seq[0, 0, y_hat] = 1\n",
    "        return ' '.join(sentence.split()[:-1])\n",
    "    \n",
    "    def top_k_sampling(self,f):\n",
    "        top_k = 2\n",
    "        en_initial_states = self.inf_encoder_model.init_states(1)\n",
    "        output, de_state_h,de_state_c = self.inf_encoder_model(f.reshape(-1, 80, 4096),en_initial_states)\n",
    "        target_seq = np.zeros((1, 1, 1500))\n",
    "        sentence = ''\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "\n",
    "        for i in range(config.max_length):\n",
    "            output_tokens, de_state_h, de_state_c, alignment = self.decoder(target_seq, [de_state_h,de_state_c],output)\n",
    "            output_tokens = tf.reshape(output_tokens,self.num_decoder_tokens)#.reshape((self.num_decoder_tokens))\n",
    "            top_index = np.argsort(output_tokens)[-top_k:]\n",
    "            top_prob = np.sort(output_tokens)[-top_k:]\n",
    "            y_hat =random.choices(top_index, weights=top_prob, k=1)[0]\n",
    "            if y_hat == 0:\n",
    "                continue\n",
    "            if self.tokenizer.index_word[y_hat] is None:\n",
    "                break\n",
    "            else:\n",
    "                sentence = sentence + self.tokenizer.index_word[y_hat] + ' '\n",
    "                if self.tokenizer.index_word[y_hat] =='eos':\n",
    "                    break\n",
    "                target_seq = np.zeros((1, 1, 1500))\n",
    "                target_seq[0, 0, y_hat] = 1\n",
    "        return ' '.join(sentence.split()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85edbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Video2Text(encoder,decoder,tokenizer,save_model_path)\n",
    "#c.test('beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a568ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.test('topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b122948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video",
   "language": "python",
   "name": "video"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
